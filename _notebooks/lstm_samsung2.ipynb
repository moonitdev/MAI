{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [LSTM과 FinanceDataReader API를 활용한 삼성전자 주가 예측](https://teddylee777.github.io/tensorflow/lstm-stock-forecast)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import FinanceDataReader as fdr\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "egyptian-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삼성전자 주식코드: 005930\n",
    "STOCK_CODE = '005930'\n",
    "stock = fdr.DataReader(STOCK_CODE)\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contrary-japanese",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock['Year'] = stock.index.year\n",
    "stock['Month'] = stock.index.month\n",
    "stock['Day'] = stock.index.day\n",
    "stock.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "sns.lineplot(y=stock['Close'], x=stock.index)\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = [['1990', '2000'], \n",
    "              ['2000', '2010'], \n",
    "              ['2010', '2015'], \n",
    "              ['2015', '2020']]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2)\n",
    "fig.set_size_inches(16, 9)\n",
    "for i in range(4):\n",
    "    ax = axes[i//2, i%2]\n",
    "    df = stock.loc[(stock.index > time_steps[i][0]) & (stock.index < time_steps[i][1])]\n",
    "    sns.lineplot(y=df['Close'], x=df.index, ax=ax)\n",
    "    ax.set_title(f'{time_steps[i][0]}~{time_steps[i][1]}')\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_ylabel('price')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-element",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 주가 데이터에 대하여 딥러닝 모델이 더 잘 학습할 수 있도록 정규화(Normalization)를 해주도록 하겠습니다.\n",
    "\n",
    "# 표준화 (Standardization)와 정규화(Normalization)에 대한 내용은 아래 링크에서 더 자세히 다루니, 참고해 보시기 바랍니다.\n",
    "\n",
    "# [데이터 전처리에 관하여](https://teddylee777.github.io/scikit-learn/scikit-learn-preprocessing)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# 스케일을 적용할 column을 정의합니다.\n",
    "scale_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "# 스케일 후 columns\n",
    "scaled = scaler.fit_transform(stock[scale_cols])\n",
    "scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(scaled, columns=scale_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  train / test 분할\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(df.drop('Close', 1), df['Close'], test_size=0.2, random_state=0, shuffle=False)\n",
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distributed-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensroFlow Dataset을 활용한 시퀀스 데이터셋 구성\n",
    "import tensorflow as tf\n",
    "\n",
    "def windowed_dataset(series, window_size, batch_size, shuffle):\n",
    "    series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "    ds = ds.map(lambda w: (w[:-1], w[-1]))\n",
    "    return ds.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-flesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter를 정의합니다.\n",
    "\n",
    "WINDOW_SIZE=20\n",
    "BATCH_SIZE=32\n",
    "# trian_data는 학습용 데이터셋, test_data는 검증용 데이터셋 입니다.\n",
    "train_data = windowed_dataset(y_train, WINDOW_SIZE, BATCH_SIZE, True)\n",
    "test_data = windowed_dataset(y_test, WINDOW_SIZE, BATCH_SIZE, False)\n",
    "# 아래의 코드로 데이터셋의 구성을 확인해 볼 수 있습니다.\n",
    "# X: (batch_size, window_size, feature)\n",
    "# Y: (batch_size, feature)\n",
    "for data in train_data.take(1):\n",
    "    print(f'데이터셋(X) 구성(batch_size, window_size, feature갯수): {data[0].shape}')\n",
    "    print(f'데이터셋(Y) 구성(batch_size, window_size, feature갯수): {data[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-credits",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, Lambda\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    # 1차원 feature map 생성\n",
    "    Conv1D(filters=32, kernel_size=5,\n",
    "           padding=\"causal\",\n",
    "           activation=\"relu\",\n",
    "           input_shape=[WINDOW_SIZE, 1]),\n",
    "    # LSTM\n",
    "    LSTM(16, activation='tanh'),\n",
    "    Dense(16, activation=\"relu\"),\n",
    "    Dense(1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence 학습에 비교적 좋은 퍼포먼스를 내는 Huber()를 사용합니다.\n",
    "loss = Huber()\n",
    "optimizer = Adam(0.0005)\n",
    "model.compile(loss=Huber(), optimizer=optimizer, metrics=['mse'])\n",
    "# earlystopping은 10번 epoch통안 val_loss 개선이 없다면 학습을 멈춥니다.\n",
    "earlystopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "# val_loss 기준 체크포인터도 생성합니다.\n",
    "filename = os.path.join('tmp', 'ckeckpointer.ckpt')\n",
    "checkpoint = ModelCheckpoint(filename, \n",
    "                             save_weights_only=True, \n",
    "                             save_best_only=True, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1)\n",
    "history = model.fit(train_data, \n",
    "                    validation_data=(test_data), \n",
    "                    epochs=50, \n",
    "                    callbacks=[checkpoint, earlystopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 ModelCheckpoint 를 로드합니다.\n",
    "\n",
    "model.load_weights(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-russian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data를 활용하여 예측을 진행합니다.\n",
    "\n",
    "pred = model.predict(test_data)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-israeli",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 9))\n",
    "plt.plot(np.asarray(y_test)[20:], label='actual')\n",
    "plt.plot(pred, label='prediction')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-forum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAI",
   "language": "python",
   "name": "mai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
